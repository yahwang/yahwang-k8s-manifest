#
# Copyright 2024 The Kubeflow authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default values for spark-operator.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride: ""
fullnameOverride: ""

commonLabels: {}

image:
  registry: docker.io
  repository: kubeflow/spark-operator
  tag: ""
  pullPolicy: IfNotPresent
  pullSecrets: []

controller:
  replicas: 1

  leaderElection:
    enable: true

  # -- Reconcile concurrency, higher values might increase memory usage.
  workers: 10

  logLevel: info

  # -- Grace period after a successful spark-submit when driver pod not found errors will be retried. Useful if the driver pod can take some time to be created.
  driverPodCreationGracePeriod: 10s

  # -- Specifies the maximum number of Executor pods that can be tracked by the controller per SparkApplication.
  maxTrackedExecutorPerApp: 1000

  uiService:
    # -- Specifies whether to create service for Spark web UI.
    enable: true

  uiIngress:
    # -- Specifies whether to create ingress for Spark web UI.
    # `controller.uiService.enable` must be `true` to enable ingress.
    enable: true
    # -- Ingress URL format.
    # Required if `controller.uiIngress.enable` is true.
    urlFormat: "spark-ui.172.30.1.101.nip.io/{{$appNamespace}}/{{$appName}}"
    # -- Optionally set the ingressClassName.
    ingressClassName: "nginx"

  batchScheduler:
    # -- Specifies whether to enable batch scheduler for spark jobs scheduling.
    # If enabled, users can specify batch scheduler name in spark application.
    enable: false
    # -- Specifies a list of kube-scheduler names for scheduling Spark pods.
    kubeSchedulerNames: []
    # - default-scheduler
    # -- Default batch scheduler to be used if not specified by the user.
    # If specified, this value must be either "volcano" or "yunikorn". Specifying any other
    # value will cause the controller to error on startup.
    default: ""

  serviceAccount:
    create: true
    name: ""
    annotations: {}
    automountServiceAccountToken: true

  rbac:
    create: true
    annotations: {}

  labels: {}

  annotations: {}

  volumes:
  - name: tmp
    emptyDir:
      sizeLimit: 1Gi

  nodeSelector: {}

  affinity: {}

  tolerations: []

  priorityClassName: ""

  podSecurityContext:
    fsGroup: 185

  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: topology.kubernetes.io/zone
  #   whenUnsatisfiable: ScheduleAnyway
  # - maxSkew: 1
  #   topologyKey: kubernetes.io/hostname
  #   whenUnsatisfiable: DoNotSchedule

  env: []

  envFrom: []

  volumeMounts:
  - name: tmp
    mountPath: "/tmp"
    readOnly: false

  # -- Pod resource requests and limits for controller containers.
  # Note, that each job submission will spawn a JVM within the controller pods using "/usr/local/openjdk-11/bin/java -Xmx128m".
  # Kubernetes may kill these Java processes at will to enforce resource limits. When that happens, you will see the following error:
  # 'failed to run spark-submit for SparkApplication [...]: signal: killed' - when this happens, you may want to increase memory limits.
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 300Mi
    # requests:
    #   cpu: 100m
    #   memory: 300Mi

  securityContext:
    readOnlyRootFilesystem: true
    privileged: false
    allowPrivilegeEscalation: false
    runAsNonRoot: true
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault

  sidecars: []

  podDisruptionBudget:
    enable: false
    minAvailable: 1

  pprof:
    enable: false
    port: 6060
    portName: pprof

  workqueueRateLimiter:
    bucketQPS: 50
    bucketSize: 500
    maxDelay:
      enable: true
      duration: 6h

webhook:
  enable: true
  replicas: 1

  leaderElection:
    enable: true

  logLevel: info

  port: 9443

  portName: webhook

  failurePolicy: Fail

  timeoutSeconds: 10

  resourceQuotaEnforcement:
    enable: false

  serviceAccount:
    create: true
    name: ""
    annotations: {}
    automountServiceAccountToken: true

  rbac:
    create: true
    annotations: {}

  labels: {}

  annotations: {}

  sidecars: []

  volumes:
  - name: serving-certs
    emptyDir:
      sizeLimit: 500Mi

  nodeSelector: {}

  affinity: {}

  tolerations: []
  priorityClassName: ""

  podSecurityContext:
    fsGroup: 185

  topologySpreadConstraints: []
  env: []
  envFrom: []

  volumeMounts:
  - name: serving-certs
    mountPath: /etc/k8s-webhook-server/serving-certs
    subPath: serving-certs
    readOnly: false

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 300Mi
    # requests:
    #   cpu: 100m
    #   memory: 300Mi

  securityContext:
    readOnlyRootFilesystem: true
    privileged: false
    allowPrivilegeEscalation: false
    runAsNonRoot: true
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault

  podDisruptionBudget:
    enable: false
    minAvailable: 1

spark:
  # -- List of namespaces where to run spark jobs.
  # If empty string is included, all namespaces will be allowed.
  # Make sure the namespaces have already existed.
  jobNamespaces:
    - spark-jobs

  serviceAccount:
    create: true
    name: ""
    annotations: {}
    automountServiceAccountToken: true

  rbac:
    create: true
    annotations: {}

prometheus:
  metrics:
    enable: true
    port: 8080
    portName: metrics
    endpoint: /metrics
    prefix: ""
    jobStartLatencyBuckets: "30,60,90,120,150,180,210,240,270,300"

  # Prometheus pod monitor for controller pods
  podMonitor:
    create: true
    labels: {}
    jobLabel: spark-operator-podmonitor
    podMetricsEndpoint:
      scheme: http
      interval: 5s
